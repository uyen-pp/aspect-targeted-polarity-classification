{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('python37': conda)",
   "metadata": {
    "interpreter": {
     "hash": "da2a2dd739486b98df65bba8b6599fd6aa55ff0b355cb46f8657e6daa711dc7a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This notebook is for testing of:\n",
    "- Data transformation from a \"cleaned, preprocessed, splited\" pickle file \n",
    "- Model loading\n",
    "- Model training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(PROJECT_DIR)\n",
    "sys.path.append(os.path.join(PROJECT_DIR, \"finetuning_and_classification\"))\n",
    "\n",
    "\n",
    "import utils\n",
    "from utils_glue import (compute_metrics, convert_examples_to_features,\n",
    "                        output_modes, processors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([\"' ng√†y u·ªëng ƒë∆∞·ª£c 3 c·ªØ th√¨ t·ªët nh·∫•t ch·ªã ·∫° '\",\n",
       "  \"' Th∆°m ngon b·ªï_d∆∞·ª°ng '\",\n",
       "  \"' Th∆°m ngon b·ªï_d∆∞·ª°ng '\",\n",
       "  \"' Minh_Anh LeƒëuÃõÃÄng queÃÇn gheÃÅ ngay cuÃõÃâa haÃÄng GiaÃÇÃÅc moÃõ suÃõÃÉa VieÃ£ÃÇt ƒëeÃÇÃâ mua ngay suÃõÃÉa Yoko ƒëeÃÇÃâ rinh nhuÃõÃÉng moÃÅn quaÃÄ ƒëaÃÅng yeÃÇu cho beÃÅ nhaüòÑ '\",\n",
       "  \"' Hay qu√° '\"],\n",
       " [[('Th√†nh_ph·∫ßn', 'POS')],\n",
       "  [('Dinh_d∆∞·ª°ng', 'POS')],\n",
       "  [('H∆∞∆°ng_v·ªã', 'POS')],\n",
       "  [('Qu√† t·∫∑ng', 'POS')],\n",
       "  [('Th∆∞∆°ng_m·∫°i_ƒëi·ªán_t·ª≠', 'POS')]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Test data load from pkl\n",
    "# Test function YNmilk_data_reform\n",
    "\n",
    "sentences, aspect_term_sentiments =utils.YNmilk_data_reform(\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\data_splited\\\\train.pickle\")\n",
    "\n",
    "sentences[0:5], aspect_term_sentiments[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/transformed/yndata\nC:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\train.pickle\nDataset: Train\n#Sentences with minimum 1 label 178065\nLabel Counts [('POS', 145849), ('NEU', 29108), ('NEG', 3108)]\n#SentencePairs 178065\nPOS% 0.8190773032319658\nNEG% 0.01745430039592284\nNEU% 0.1634683963721113\nPOS/NEG 46.926962676962674\nPOS/NEU 5.010615638312491\nNEG/NEU 0.10677476982272914\n\nDataset: Dev\n#Sentences with minimum 1 label 17806\nLabel Counts [('POS', 14652), ('NEU', 2850), ('NEG', 304)]\n#SentencePairs 17806\nPOS% 0.822868695945187\nNEG% 0.017072896776367516\nNEU% 0.16005840727844547\nPOS/NEG 48.19736842105263\nPOS/NEU 5.1410526315789475\nNEG/NEU 0.10666666666666667\n\nDataset: TrainSplit\n#Sentences with minimum 1 label 160259\nLabel Counts [('POS', 131197), ('NEU', 26258), ('NEG', 2804)]\n#SentencePairs 160259\nPOS% 0.8186560505182237\nNEG% 0.01749667725369558\nNEU% 0.1638472722280808\nPOS/NEG 46.78922967189729\nPOS/NEU 4.9964582222560745\nNEG/NEU 0.10678650316094143\n\n"
     ]
    }
   ],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "!python prepare_YNmilk_datasets.py \\\n",
    "    --files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\train.pickle\" \\\n",
    "    --output_dir data/transformed/yndata \\\n",
    "    --istrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/transformed/yndata\n",
      "C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\validate.pickle\n",
      "Dataset: Test\n",
      "#Sentences with minimum 1 label 38427\n",
      "Label Counts [('POS', 31555), ('NEU', 6166), ('NEG', 706)]\n",
      "#SentencePairs 38427\n",
      "POS% 0.8211674083326828\n",
      "NEG% 0.018372498503656284\n",
      "NEU% 0.16046009316366097\n",
      "POS/NEG 44.695467422096314\n",
      "POS/NEU 5.117580278949076\n",
      "NEG/NEU 0.11449886474213429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python prepare_YNmilk_datasets.py \\\n",
    "--files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\validate.pickle\" \\\n",
    "--output_dir data/transformed/yndata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "03/02/2021 12:45:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\nSome weights of the model checkpoint at C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n03/02/2021 12:46:04 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\\\\data/transformed/yndata', device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=3e-05, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=128, max_steps=800, model_name_or_path='C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base', model_type='phobert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=8, save_steps=200, seed=42, server_ip='', server_port='', task_name='atsc', tokenizer_name='', warmup_steps=120, weight_decay=0.0)\n03/02/2021 12:46:04 - INFO - __main__ -   Loading features from cached file C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\\data/transformed/yndata\\cached_train_huggingface_phobert_base_128_atsc\n03/02/2021 12:46:10 - INFO - utils_glue -   Writing example 0 of 178065\n03/02/2021 12:46:10 - INFO - utils_glue -   *** Example ***\n03/02/2021 12:46:10 - INFO - utils_glue -   guid: train-0\n03/02/2021 12:46:10 - INFO - utils_glue -   tokens: <s> ' ng√†y u·ªëng ƒë∆∞·ª£c 3 c·ªØ th√¨ t·ªët nh·∫•t ch·ªã ·∫° ' </s> Th√†nh_ph·∫ßn </s>\n03/02/2021 12:46:10 - INFO - utils_glue -   input_ids: 0 104 43 653 11 107 35417 54 167 67 213 3628 104 2 12744 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   label: POS (id = 0)\n03/02/2021 12:46:10 - INFO - utils_glue -   *** Example ***\n03/02/2021 12:46:10 - INFO - utils_glue -   guid: train-1\n03/02/2021 12:46:10 - INFO - utils_glue -   tokens: <s> ' Th∆°m ngon b·ªï_d∆∞·ª°ng ' </s> H∆∞∆°ng_v·ªã </s>\n03/02/2021 12:46:10 - INFO - utils_glue -   input_ids: 0 104 10474 1325 10130 104 2 33191 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   label: POS (id = 0)\n03/02/2021 12:46:10 - INFO - utils_glue -   *** Example ***\n03/02/2021 12:46:10 - INFO - utils_glue -   guid: train-2\n03/02/2021 12:46:10 - INFO - utils_glue -   tokens: <s> ' Th∆°m ngon b·ªï_d∆∞·ª°ng ' </s> Dinh_d∆∞·ª°ng </s>\n03/02/2021 12:46:10 - INFO - utils_glue -   input_ids: 0 104 10474 1325 10130 104 2 13232 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   label: POS (id = 0)\n03/02/2021 12:46:10 - INFO - utils_glue -   *** Example ***\n03/02/2021 12:46:10 - INFO - utils_glue -   guid: train-3\n03/02/2021 12:46:10 - INFO - utils_glue -   tokens: <s> ' Minh_Anh Le ƒëu@@ Ãõ@@ ÃÄ@@ ng que@@ ÃÇ@@ n ghe@@ ÃÅ ngay cu@@ Ãõ@@ Ãâ@@ a ha@@ ÃÄ@@ ng Gia@@ ÃÇ@@ ÃÅ@@ c mo@@ Ãõ su@@ Ãõ@@ ÃÉ@@ a Vie@@ Ã£@@ ÃÇ@@ t ƒëe@@ ÃÇ@@ Ãâ mua ngay su@@ Ãõ@@ ÃÉ@@ a Yo@@ ko ƒëe@@ ÃÇ@@ Ãâ rinh nhu@@ Ãõ@@ ÃÉ@@ ng mo@@ ÃÅ@@ n qua@@ ÃÄ ƒëa@@ ÃÅ@@ ng ye@@ ÃÇ@@ u cho be@@ ÃÅ nha@@ üòÑ ' </s> Qu√† t·∫∑ng </s>\n03/02/2021 12:46:10 - INFO - utils_glue -   input_ids: 0 104 9357 18126 26728 3 58216 2795 27262 3 1677 37660 60455 249 5109 3 60198 1517 8216 58216 2795 34426 3 45729 1894 3773 3 3611 3 61685 1517 35559 55479 3 1204 34463 3 61152 188 249 3611 3 61685 1517 9674 5516 34463 3 61152 25324 24913 3 61685 2795 3773 45729 1677 38937 61538 20327 45729 2795 10081 3 1656 13 5329 60455 37361 3 104 2 18664 806 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   label: POS (id = 0)\n03/02/2021 12:46:10 - INFO - utils_glue -   *** Example ***\n03/02/2021 12:46:10 - INFO - utils_glue -   guid: train-4\n03/02/2021 12:46:10 - INFO - utils_glue -   tokens: <s> ' Hay qu√° ' </s> N·ªôi_dung Ti·∫øp_th·ªã ƒëi·ªán_t·ª≠ </s>\n03/02/2021 12:46:10 - INFO - utils_glue -   input_ids: 0 104 3896 204 104 2 3947 32370 1143 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/02/2021 12:46:10 - INFO - utils_glue -   label: POS (id = 0)\n03/02/2021 12:46:13 - INFO - utils_glue -   Writing example 10000 of 178065\n03/02/2021 12:46:16 - INFO - utils_glue -   Writing example 20000 of 178065\n03/02/2021 12:46:18 - INFO - utils_glue -   Writing example 30000 of 178065\n03/02/2021 12:46:21 - INFO - utils_glue -   Writing example 40000 of 178065\n03/02/2021 12:46:24 - INFO - utils_glue -   Writing example 50000 of 178065\n03/02/2021 12:46:26 - INFO - utils_glue -   Writing example 60000 of 178065\n03/02/2021 12:46:29 - INFO - utils_glue -   Writing example 70000 of 178065\n03/02/2021 12:46:32 - INFO - utils_glue -   Writing example 80000 of 178065\n03/02/2021 12:46:34 - INFO - utils_glue -   Writing example 90000 of 178065\n03/02/2021 12:46:37 - INFO - utils_glue -   Writing example 100000 of 178065\n03/02/2021 12:46:40 - INFO - utils_glue -   Writing example 110000 of 178065\n03/02/2021 12:46:42 - INFO - utils_glue -   Writing example 120000 of 178065\n03/02/2021 12:46:45 - INFO - utils_glue -   Writing example 130000 of 178065\n03/02/2021 12:46:47 - INFO - utils_glue -   Writing example 140000 of 178065\n03/02/2021 12:46:50 - INFO - utils_glue -   Writing example 150000 of 178065\n03/02/2021 12:46:53 - INFO - utils_glue -   Writing example 160000 of 178065\n03/02/2021 12:46:55 - INFO - utils_glue -   Writing example 170000 of 178065\n03/02/2021 12:47:00 - INFO - __main__ -   ***** Running training *****\n03/02/2021 12:47:00 - INFO - __main__ -     Num examples = 178065\n03/02/2021 12:47:00 - INFO - __main__ -     Num Epochs = 1\n03/02/2021 12:47:00 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n03/02/2021 12:47:00 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n03/02/2021 12:47:00 - INFO - __main__ -     Gradient Accumulation steps = 1\n03/02/2021 12:47:00 - INFO - __main__ -     Total optimization steps = 800\n\nEpoch:   0%|          | 0/1 [00:00<?, ?it/s]\n\nIteration:   0%|          | 0/22259 [00:00<?, ?it/s]\u001b[A\n\nIteration:   0%|          | 1/22259 [00:01<11:15:43,  1.82s/it]\u001b[A\nIteration:   0%|          | 1/22259 [00:02<16:02:52,  2.60s/it]\n\nEpoch:   0%|          | 0/1 [00:02<?, ?it/s]\nTraceback (most recent call last):\n  File \"./finetuning_and_classification/run_glue.py\", line 512, in <module>\n    main()\n  File \"./finetuning_and_classification/run_glue.py\", line 466, in main\n    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n  File \"./finetuning_and_classification/run_glue.py\", line 122, in train\n    ouputs = model(**inputs)\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 1137, in forward\n    outputs = self.roberta(\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 798, in forward\n    encoder_outputs = self.encoder(\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 498, in forward\n    layer_outputs = layer_module(\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 393, in forward\n    self_attention_outputs = self.attention(\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 321, in forward\n    self_outputs = self.self(\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Uyen\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 257, in forward\n    context_layer = torch.matmul(attention_probs, value_layer)\nRuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.74 GiB already allocated; 17.23 MiB free; 2.90 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "MODEL_DIR = os.path.join(PROJECT_DIR, \"data/models/phobert_base/huggingface_phobert_base\")\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"data/transformed/yndata\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_DIR, \"YNData-ar-phobert-milk\")\n",
    "\n",
    "!python ./finetuning_and_classification/run_glue.py \\\n",
    "    --model_type=\"phobert\" \\\n",
    "    --model_name_or_path=C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base \\\n",
    "    --do_train \\\n",
    "    # --evaluate_during_training \\\n",
    "    --logging_steps 100 --save_steps 200 --task_name=\"atsc\" \\\n",
    "    --seed 42 \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir=\"$DATA_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --max_seq_length=128 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --per_gpu_eval_batch_size=32 \\\n",
    "    --gradient_accumulation_steps=1 \\\n",
    "    --max_steps=800 \\\n",
    "    --overwrite_output_dir --overwrite_cache --warmup_steps=120\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "\n",
    "\n",
    "# config_class, model_class, tokenizer_class = (BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "# config = config_class.from_pretrained(MODEL_DIR, num_labels=num_labels, finetuning_task=task_name)\n",
    "# tokenizer = tokenizer_class.from_pretrained(MODEL_DIR, do_lower_case=True)\n",
    "# model = model_class.from_pretrained(MODEL_DIR, from_tf=bool('.ckpt' in MODEL_DIR), config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/transformed/yndata/testdata\nC:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\test.pickle\nDataset: Test\n#Sentences with minimum 1 label 38043\nLabel Counts [('POS', 30960), ('NEU', 6437), ('NEG', 646)]\n#SentencePairs 38043\nPOS% 0.8138159451147385\nNEG% 0.0169807849012959\nNEU% 0.1692032699839655\nPOS/NEG 47.925696594427244\nPOS/NEU 4.80969395681218\nNEG/NEU 0.10035730930557714\n\n"
     ]
    }
   ],
   "source": [
    "!python prepare_YNmilk_datasets.py \\\n",
    "--files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\test.pickle\" \\\n",
    "--output_dir data/transformed/yndata/testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ing:   6%|‚ñå         | 74/1189 [00:43<10:38,  1.75it/s]\n",
      "Evaluating:   6%|‚ñã         | 75/1189 [00:43<10:38,  1.74it/s]\n",
      "Evaluating:   6%|‚ñã         | 76/1189 [00:44<10:39,  1.74it/s]\n",
      "Evaluating:   6%|‚ñã         | 77/1189 [00:44<10:38,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 78/1189 [00:45<10:38,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 79/1189 [00:45<10:36,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 80/1189 [00:46<10:37,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 81/1189 [00:47<10:35,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 82/1189 [00:47<10:34,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 83/1189 [00:48<10:34,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 84/1189 [00:48<10:34,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 85/1189 [00:49<10:34,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 86/1189 [00:49<10:33,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 87/1189 [00:50<10:33,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 88/1189 [00:51<10:32,  1.74it/s]\n",
      "Evaluating:   7%|‚ñã         | 89/1189 [00:51<10:30,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 90/1189 [00:52<10:30,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 91/1189 [00:52<10:29,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 92/1189 [00:53<10:27,  1.75it/s]\n",
      "Evaluating:   8%|‚ñä         | 93/1189 [00:53<10:29,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 94/1189 [00:54<10:29,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 95/1189 [00:55<10:28,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 96/1189 [00:55<10:27,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 97/1189 [00:56<10:26,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 98/1189 [00:56<10:26,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 99/1189 [00:57<10:24,  1.74it/s]\n",
      "Evaluating:   8%|‚ñä         | 100/1189 [00:57<10:23,  1.75it/s]\n",
      "Evaluating:   8%|‚ñä         | 101/1189 [00:58<10:23,  1.75it/s]\n",
      "Evaluating:   9%|‚ñä         | 102/1189 [00:59<10:22,  1.75it/s]\n",
      "Evaluating:   9%|‚ñä         | 103/1189 [00:59<10:22,  1.75it/s]\n",
      "Evaluating:   9%|‚ñä         | 104/1189 [01:00<10:21,  1.74it/s]\n",
      "Evaluating:   9%|‚ñâ         | 105/1189 [01:00<10:21,  1.75it/s]\n",
      "Evaluating:   9%|‚ñâ         | 106/1189 [01:01<10:20,  1.75it/s]\n",
      "Evaluating:   9%|‚ñâ         | 107/1189 [01:01<10:20,  1.75it/s]\n",
      "Evaluating:   9%|‚ñâ         | 108/1189 [01:02<10:20,  1.74it/s]\n",
      "Evaluating:   9%|‚ñâ         | 109/1189 [01:03<10:19,  1.74it/s]\n",
      "Evaluating:   9%|‚ñâ         | 110/1189 [01:03<10:19,  1.74it/s]\n",
      "Evaluating:   9%|‚ñâ         | 111/1189 [01:04<10:19,  1.74it/s]\n",
      "Evaluating:   9%|‚ñâ         | 112/1189 [01:04<10:18,  1.74it/s]\n",
      "Evaluating:  10%|‚ñâ         | 113/1189 [01:05<10:17,  1.74it/s]\n",
      "Evaluating:  10%|‚ñâ         | 114/1189 [01:05<10:16,  1.74it/s]\n",
      "Evaluating:  10%|‚ñâ         | 115/1189 [01:06<10:15,  1.74it/s]\n",
      "Evaluating:  10%|‚ñâ         | 116/1189 [01:07<10:15,  1.74it/s]\n",
      "Evaluating:  10%|‚ñâ         | 117/1189 [01:07<10:15,  1.74it/s]\n",
      "Evaluating:  10%|‚ñâ         | 118/1189 [01:08<10:14,  1.74it/s]\n",
      "Evaluating:  10%|‚ñà         | 119/1189 [01:08<10:14,  1.74it/s]\n",
      "Evaluating:  10%|‚ñà         | 120/1189 [01:09<10:14,  1.74it/s]\n",
      "Evaluating:  10%|‚ñà         | 121/1189 [01:10<10:13,  1.74it/s]\n",
      "Evaluating:  10%|‚ñà         | 122/1189 [01:10<10:12,  1.74it/s]\n",
      "Evaluating:  10%|‚ñà         | 123/1189 [01:11<10:11,  1.74it/s]\n",
      "Evaluating:  10%|‚ñà         | 124/1189 [01:11<10:10,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà         | 125/1189 [01:12<10:11,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà         | 126/1189 [01:12<10:09,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà         | 127/1189 [01:13<10:08,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà         | 128/1189 [01:14<10:09,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà         | 129/1189 [01:14<10:08,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà         | 130/1189 [01:15<10:08,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà         | 131/1189 [01:15<10:07,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà         | 132/1189 [01:16<10:05,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà         | 133/1189 [01:16<10:05,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà‚ñè        | 134/1189 [01:17<10:05,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà‚ñè        | 135/1189 [01:18<10:04,  1.74it/s]\n",
      "Evaluating:  11%|‚ñà‚ñè        | 136/1189 [01:18<10:04,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 137/1189 [01:19<10:03,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 138/1189 [01:19<10:03,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 139/1189 [01:20<10:02,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 140/1189 [01:20<10:01,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 141/1189 [01:21<10:01,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 142/1189 [01:22<10:00,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 143/1189 [01:22<10:00,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 144/1189 [01:23<09:59,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 145/1189 [01:23<09:59,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 146/1189 [01:24<09:59,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 147/1189 [01:24<09:58,  1.74it/s]\n",
      "Evaluating:  12%|‚ñà‚ñè        | 148/1189 [01:25<09:57,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 149/1189 [01:26<09:56,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 150/1189 [01:26<09:55,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 151/1189 [01:27<09:55,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 152/1189 [01:27<09:54,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 153/1189 [01:28<09:54,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 154/1189 [01:28<09:53,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 155/1189 [01:29<09:53,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 156/1189 [01:30<09:54,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 157/1189 [01:30<09:53,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 158/1189 [01:31<09:52,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 159/1189 [01:31<09:51,  1.74it/s]\n",
      "Evaluating:  13%|‚ñà‚ñé        | 160/1189 [01:32<09:51,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñé        | 161/1189 [01:32<09:49,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñé        | 162/1189 [01:33<09:49,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñé        | 163/1189 [01:34<09:48,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñç        | 164/1189 [01:34<09:48,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñç        | 165/1189 [01:35<09:47,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñç        | 166/1189 [01:35<09:46,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñç        | 167/1189 [01:36<09:46,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñç        | 168/1189 [01:36<09:45,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñç        | 169/1189 [01:37<09:44,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñç        | 170/1189 [01:38<09:45,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñç        | 171/1189 [01:38<09:45,  1.74it/s]\n",
      "Evaluating:  14%|‚ñà‚ñç        | 172/1189 [01:39<09:45,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñç        | 173/1189 [01:39<09:45,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñç        | 174/1189 [01:40<09:44,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñç        | 175/1189 [01:41<09:43,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñç        | 176/1189 [01:41<09:43,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñç        | 177/1189 [01:42<09:43,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñç        | 178/1189 [01:42<09:42,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñå        | 179/1189 [01:43<09:41,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñå        | 180/1189 [01:43<09:41,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñå        | 181/1189 [01:44<09:40,  1.74it/s]\n",
      "Evaluating:  15%|‚ñà‚ñå        | 182/1189 [01:45<09:40,  1.73it/s]\n",
      "Evaluating:  15%|‚ñà‚ñå        | 183/1189 [01:45<09:39,  1.73it/s]\n",
      "Evaluating:  15%|‚ñà‚ñå        | 184/1189 [01:46<09:39,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñå        | 185/1189 [01:46<09:39,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñå        | 186/1189 [01:47<09:38,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñå        | 187/1189 [01:47<09:37,  1.74it/s]\n",
      "Evaluating:  16%|‚ñà‚ñå        | 188/1189 [01:48<09:37,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñå        | 189/1189 [01:49<09:37,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñå        | 190/1189 [01:49<09:36,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñå        | 191/1189 [01:50<09:35,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñå        | 192/1189 [01:50<09:34,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñå        | 193/1189 [01:51<09:34,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñã        | 194/1189 [01:51<09:33,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñã        | 195/1189 [01:52<09:33,  1.73it/s]\n",
      "Evaluating:  16%|‚ñà‚ñã        | 196/1189 [01:53<09:32,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 197/1189 [01:53<09:32,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 198/1189 [01:54<09:32,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 199/1189 [01:54<09:32,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 200/1189 [01:55<09:31,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 201/1189 [01:56<09:30,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 202/1189 [01:56<09:30,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 203/1189 [01:57<09:28,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 204/1189 [01:57<09:27,  1.74it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 205/1189 [01:58<09:27,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 206/1189 [01:58<09:26,  1.74it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 207/1189 [01:59<09:26,  1.73it/s]\n",
      "Evaluating:  17%|‚ñà‚ñã        | 208/1189 [02:00<09:25,  1.73it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 209/1189 [02:00<09:24,  1.74it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 210/1189 [02:01<09:23,  1.74it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 211/1189 [02:01<09:23,  1.73it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 212/1189 [02:02<09:18,  1.75it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 213/1189 [02:02<09:23,  1.73it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 214/1189 [02:03<09:22,  1.73it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 215/1189 [02:04<09:22,  1.73it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 216/1189 [02:04<09:21,  1.73it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 217/1189 [02:05<09:21,  1.73it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 218/1189 [02:05<09:20,  1.73it/s]\n",
      "Evaluating:  18%|‚ñà‚ñä        | 219/1189 [02:06<09:19,  1.73it/s]\n",
      "Evaluating:  19%|‚ñà‚ñä        | 220/1189 [02:06<09:17,  1.74it/s]\n",
      "Evaluating:  19%|‚ñà‚ñä        | 221/1189 [02:07<09:17,  1.74it/s]\n",
      "Evaluating:  19%|‚ñà‚ñä        | 222/1189 [02:08<09:16,  1.74it/s]\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 223/1189 [02:08<09:17,  1.73it/s]\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 224/1189 [02:09<09:17,  1.73it/s]\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 225/1189 [02:09<09:15,  1.73it/s]\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 226/1189 [02:10<09:16,  1.73it/s]\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 227/1189 [02:10<09:13,  1.74it/s]\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 228/1189 [02:11<09:15,  1.73it/s]\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 229/1189 [02:12<09:11,  1.74it/s]\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 230/1189 [02:12<09:13,  1.73it/s]\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 231/1189 [02:13<09:13,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñâ        | 232/1189 [02:13<09:12,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñâ        | 233/1189 [02:14<09:12,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñâ        | 234/1189 [02:15<09:11,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñâ        | 235/1189 [02:15<09:10,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñâ        | 236/1189 [02:16<09:09,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñâ        | 237/1189 [02:16<09:08,  1.74it/s]\n",
      "Evaluating:  20%|‚ñà‚ñà        | 238/1189 [02:17<09:08,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñà        | 239/1189 [02:17<09:07,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñà        | 240/1189 [02:18<09:07,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñà        | 241/1189 [02:19<09:07,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñà        | 242/1189 [02:19<09:06,  1.73it/s]\n",
      "Evaluating:  20%|‚ñà‚ñà        | 243/1189 [02:20<09:06,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà        | 244/1189 [02:20<09:05,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà        | 245/1189 [02:21<09:04,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà        | 246/1189 [02:21<09:01,  1.74it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà        | 247/1189 [02:22<09:04,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà        | 248/1189 [02:23<09:03,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà        | 249/1189 [02:23<09:02,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà        | 250/1189 [02:24<09:02,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà        | 251/1189 [02:24<09:01,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà        | 252/1189 [02:25<09:00,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà‚ñè       | 253/1189 [02:26<08:59,  1.73it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà‚ñè       | 254/1189 [02:26<08:57,  1.74it/s]\n",
      "Evaluating:  21%|‚ñà‚ñà‚ñè       | 255/1189 [02:27<08:59,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 256/1189 [02:27<08:58,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 257/1189 [02:28<08:57,  1.74it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 258/1189 [02:28<08:57,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 259/1189 [02:29<08:57,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 260/1189 [02:30<08:56,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 261/1189 [02:30<08:55,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 262/1189 [02:31<08:55,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 263/1189 [02:31<08:54,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 264/1189 [02:32<08:54,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 265/1189 [02:32<08:53,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 266/1189 [02:33<08:53,  1.73it/s]\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 267/1189 [02:34<08:52,  1.73it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 268/1189 [02:34<08:51,  1.73it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 269/1189 [02:35<08:51,  1.73it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 270/1189 [02:35<08:50,  1.73it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 271/1189 [02:36<08:49,  1.73it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 272/1189 [02:36<08:48,  1.74it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 273/1189 [02:37<08:47,  1.74it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 274/1189 [02:38<08:46,  1.74it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 275/1189 [02:38<08:46,  1.73it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 276/1189 [02:39<08:46,  1.73it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 277/1189 [02:39<08:46,  1.73it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 278/1189 [02:40<08:46,  1.73it/s]\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 279/1189 [02:41<08:45,  1.73it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñé       | 280/1189 [02:41<08:44,  1.73it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñé       | 281/1189 [02:42<08:43,  1.74it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñé       | 282/1189 [02:42<08:42,  1.74it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 283/1189 [02:43<08:41,  1.74it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 284/1189 [02:43<08:42,  1.73it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 285/1189 [02:44<08:41,  1.73it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 286/1189 [02:45<08:40,  1.74it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 287/1189 [02:45<08:40,  1.73it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 288/1189 [02:46<08:39,  1.73it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 289/1189 [02:46<08:38,  1.74it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 290/1189 [02:47<08:37,  1.74it/s]\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 291/1189 [02:47<08:37,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñç       | 292/1189 [02:48<08:37,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñç       | 293/1189 [02:49<08:36,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñç       | 294/1189 [02:49<08:36,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñç       | 295/1189 [02:50<08:35,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñç       | 296/1189 [02:50<08:33,  1.74it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñç       | 297/1189 [02:51<08:34,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 298/1189 [02:51<08:34,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 299/1189 [02:52<08:33,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 300/1189 [02:53<08:32,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 301/1189 [02:53<08:32,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 302/1189 [02:54<08:31,  1.73it/s]\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 303/1189 [02:54<08:32,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 304/1189 [02:55<08:31,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 305/1189 [02:56<08:30,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 306/1189 [02:56<08:30,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 307/1189 [02:57<08:29,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 308/1189 [02:57<08:28,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 309/1189 [02:58<08:28,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 310/1189 [02:58<08:27,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 311/1189 [02:59<08:27,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 312/1189 [03:00<08:26,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñã       | 313/1189 [03:00<08:25,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñã       | 314/1189 [03:01<08:24,  1.73it/s]\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñã       | 315/1189 [03:01<08:24,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 316/1189 [03:02<08:23,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 317/1189 [03:02<08:23,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 318/1189 [03:03<08:23,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 319/1189 [03:04<08:22,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 320/1189 [03:04<08:21,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 321/1189 [03:05<08:21,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 322/1189 [03:05<08:20,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 323/1189 [03:06<08:20,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 324/1189 [03:06<08:19,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 325/1189 [03:07<08:18,  1.73it/s]\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 326/1189 [03:08<08:18,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 327/1189 [03:08<08:17,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 328/1189 [03:09<08:17,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 329/1189 [03:09<08:17,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 330/1189 [03:10<08:16,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 331/1189 [03:11<08:15,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 332/1189 [03:11<08:15,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 333/1189 [03:12<08:14,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 334/1189 [03:12<08:13,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 335/1189 [03:13<08:12,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 336/1189 [03:13<08:12,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 337/1189 [03:14<08:11,  1.73it/s]\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 338/1189 [03:15<08:10,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñä       | 339/1189 [03:15<08:10,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñä       | 340/1189 [03:16<08:09,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñä       | 341/1189 [03:16<08:09,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 342/1189 [03:17<08:08,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 343/1189 [03:17<08:08,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 344/1189 [03:18<08:07,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 345/1189 [03:19<08:07,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 346/1189 [03:19<08:07,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 347/1189 [03:20<08:06,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 348/1189 [03:20<08:05,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 349/1189 [03:21<08:04,  1.73it/s]\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 350/1189 [03:21<08:03,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñâ       | 351/1189 [03:22<08:03,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñâ       | 352/1189 [03:23<08:02,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñâ       | 353/1189 [03:23<08:02,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñâ       | 354/1189 [03:24<08:02,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñâ       | 355/1189 [03:24<08:02,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñâ       | 356/1189 [03:25<08:00,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 357/1189 [03:26<08:00,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 358/1189 [03:26<07:59,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 359/1189 [03:27<07:59,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 360/1189 [03:27<07:59,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 361/1189 [03:28<07:58,  1.73it/s]\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 362/1189 [03:28<07:58,  1.73it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 363/1189 [03:29<07:57,  1.73it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 364/1189 [03:30<07:56,  1.73it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 365/1189 [03:30<07:55,  1.73it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 366/1189 [03:31<07:54,  1.73it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 367/1189 [03:31<07:53,  1.74it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 368/1189 [03:32<07:53,  1.74it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 369/1189 [03:32<07:52,  1.74it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 370/1189 [03:33<07:52,  1.73it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 371/1189 [03:34<07:51,  1.73it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà‚ñè      | 372/1189 [03:34<07:51,  1.73it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà‚ñè      | 373/1189 [03:35<07:50,  1.74it/s]\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà‚ñè      | 374/1189 [03:35<07:49,  1.74it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 375/1189 [03:36<07:49,  1.74it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 376/1189 [03:36<07:49,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 377/1189 [03:37<07:48,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 378/1189 [03:38<07:47,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 379/1189 [03:38<07:47,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 380/1189 [03:39<07:46,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 381/1189 [03:39<07:46,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 382/1189 [03:40<07:45,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 383/1189 [03:41<07:45,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 384/1189 [03:41<07:44,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 385/1189 [03:42<07:44,  1.73it/s]\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 386/1189 [03:42<07:42,  1.74it/s]\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 387/1189 [03:43<07:43,  1.73it/s]\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 388/1189 [03:43<07:40,  1.74it/s]\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 389/1189 [03:44<07:43,  1.73it/s]\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 390/1189 [03:45<07:42,  1.73it/s]\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 391/1189 [03:45<07:41,  1.73it/s]"
     ]
    }
   ],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "!python ./finetuning_and_classification/run_glue.py \\\n",
    "    --model_type=\"phobert\" \\\n",
    "    --do_eval \\\n",
    "    --model_name_or_path=\"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification/data/models/finetuned_phobert4classification/\" \\\n",
    "    --task_name=\"atsc\" \\\n",
    "    --output_dir=\"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification/data/models/finetuned_phobert4classification/\"  \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir=./data/transformed/yndata/testdata \\\n",
    "    --max_seq_length=128 \\\n",
    "    --per_gpu_eval_batch_size=32 \\\n",
    "    --overwrite_output_dir --overwrite_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(\"' C√¥ng_nh·∫≠n u·ªëng Friso t·ªët cho h·ªá ti√™u_ho√° l·∫Øm lu√¥n ƒë√≥ nha c \\\\ n '\",\n",
       " 'Ch·ª©c_nƒÉng ti√™u_ho√°')"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# text parser\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "filepath = \"C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/transformed/yndata/testdata/test.xml\"\n",
    "\n",
    "with open(filepath, 'rb') as f:\n",
    "    sentence_elements = ET.parse(f).getroot().iter('sentence')\n",
    "\n",
    "e_0 = list(sentence_elements)[0]\n",
    "e_0.find('text').text, e_0.find('aspectTerms').find('aspectTerm').get('term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Ch·ª©c_nƒÉng ti√™u_ho√°'"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# Test dataloader\n",
    "filepath = \"C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/transformed/yndata/testdata/test.xml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-aa233d0ff84f88a1\n",
      "Downloading and preparing dataset yn_dataset/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\Uyen\\.cache\\huggingface\\datasets\\yn_dataset\\default-aa233d0ff84f88a1\\1.1.0\\976a607c4f36d10e025b4a64da7d52f8fe14b425fdc639546361d76033602814...\n",
      "Dataset yn_dataset downloaded and prepared to C:\\Users\\Uyen\\.cache\\huggingface\\datasets\\yn_dataset\\default-aa233d0ff84f88a1\\1.1.0\\976a607c4f36d10e025b4a64da7d52f8fe14b425fdc639546361d76033602814. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_dir=\"C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/transformed/yndata/\"\n",
    "\n",
    "dataset = load_dataset('load_dataset_ar.py', data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[32,\n",
       " 13,\n",
       " 6,\n",
       " 31,\n",
       " 27,\n",
       " 33,\n",
       " 22,\n",
       " 3,\n",
       " 3,\n",
       " 35,\n",
       " 24,\n",
       " 32,\n",
       " 22,\n",
       " 17,\n",
       " 32,\n",
       " 3,\n",
       " 31,\n",
       " 3,\n",
       " 29,\n",
       " 30,\n",
       " 19,\n",
       " 5,\n",
       " 3,\n",
       " 30,\n",
       " 31,\n",
       " 24,\n",
       " 6,\n",
       " 13,\n",
       " 17,\n",
       " 6,\n",
       " 28,\n",
       " 32,\n",
       " 17,\n",
       " 3,\n",
       " 28,\n",
       " 24,\n",
       " 6,\n",
       " 3,\n",
       " 17,\n",
       " 32,\n",
       " 12,\n",
       " 29,\n",
       " 13,\n",
       " 13,\n",
       " 24,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 16,\n",
       " 13,\n",
       " 24,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 35,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 32,\n",
       " 6,\n",
       " 30,\n",
       " 3,\n",
       " 3,\n",
       " 25,\n",
       " 5,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 3,\n",
       " 6,\n",
       " 35,\n",
       " 24,\n",
       " 6,\n",
       " 17,\n",
       " 24,\n",
       " 6,\n",
       " 24,\n",
       " 6,\n",
       " 3,\n",
       " 24,\n",
       " 6,\n",
       " 22,\n",
       " 30,\n",
       " 3,\n",
       " 25,\n",
       " 24,\n",
       " 27,\n",
       " 33,\n",
       " 24,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 13,\n",
       " 3,\n",
       " 24,\n",
       " 6,\n",
       " 24,\n",
       " 31,\n",
       " 13,\n",
       " 24,\n",
       " 32,\n",
       " 12,\n",
       " 19,\n",
       " 24,\n",
       " 25,\n",
       " 36,\n",
       " 5,\n",
       " 6,\n",
       " 16,\n",
       " 24,\n",
       " 19,\n",
       " 12,\n",
       " 3,\n",
       " 28,\n",
       " 29,\n",
       " 31,\n",
       " 3,\n",
       " 17,\n",
       " 28,\n",
       " 13,\n",
       " 32,\n",
       " 19,\n",
       " 24,\n",
       " 32,\n",
       " 12,\n",
       " 24,\n",
       " 3,\n",
       " 24,\n",
       " 6,\n",
       " 25,\n",
       " 34,\n",
       " 19,\n",
       " 17,\n",
       " 3,\n",
       " 18,\n",
       " 33,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 25,\n",
       " 3,\n",
       " 6,\n",
       " 29,\n",
       " 6,\n",
       " 3,\n",
       " 26,\n",
       " 25,\n",
       " 3,\n",
       " 24,\n",
       " 30,\n",
       " 32,\n",
       " 3,\n",
       " 19,\n",
       " 32,\n",
       " 19,\n",
       " 30,\n",
       " 24,\n",
       " 32,\n",
       " 12,\n",
       " 32,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 24,\n",
       " 32,\n",
       " 17,\n",
       " 28,\n",
       " 19,\n",
       " 32,\n",
       " 5,\n",
       " 19,\n",
       " 6,\n",
       " 12,\n",
       " 19,\n",
       " 20,\n",
       " 14,\n",
       " 3,\n",
       " 6,\n",
       " 25,\n",
       " 24,\n",
       " 35,\n",
       " 25,\n",
       " 16,\n",
       " 27,\n",
       " 33,\n",
       " 6,\n",
       " 23,\n",
       " 3,\n",
       " 6,\n",
       " 17,\n",
       " 6,\n",
       " 25,\n",
       " 6,\n",
       " 3,\n",
       " 16,\n",
       " 6,\n",
       " 3,\n",
       " 16,\n",
       " 5,\n",
       " 27,\n",
       " 33,\n",
       " 27,\n",
       " 33,\n",
       " 13,\n",
       " 29,\n",
       " 3,\n",
       " 25,\n",
       " 3,\n",
       " 36,\n",
       " 29,\n",
       " 24,\n",
       " 6,\n",
       " 22,\n",
       " 13,\n",
       " 3,\n",
       " 3,\n",
       " 17,\n",
       " 6,\n",
       " 16,\n",
       " 6,\n",
       " 24,\n",
       " 32,\n",
       " 29,\n",
       " 27,\n",
       " 33,\n",
       " 30,\n",
       " 12,\n",
       " 13,\n",
       " 17,\n",
       " 14,\n",
       " 30,\n",
       " 5,\n",
       " 28,\n",
       " 19,\n",
       " 17,\n",
       " 25,\n",
       " 13,\n",
       " 32,\n",
       " 3,\n",
       " 28,\n",
       " 24,\n",
       " 6,\n",
       " 22,\n",
       " 31,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 28,\n",
       " 25,\n",
       " 13,\n",
       " 3,\n",
       " 6,\n",
       " 22,\n",
       " 3,\n",
       " 25,\n",
       " 3,\n",
       " 16,\n",
       " 31,\n",
       " 29,\n",
       " 27,\n",
       " 33,\n",
       " 6,\n",
       " 3,\n",
       " 22,\n",
       " 31,\n",
       " 36,\n",
       " 32,\n",
       " 24,\n",
       " 32,\n",
       " 3,\n",
       " 6,\n",
       " 24,\n",
       " 6,\n",
       " 3,\n",
       " 27,\n",
       " 33,\n",
       " 24,\n",
       " 19,\n",
       " 17,\n",
       " 35,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 17,\n",
       " 33,\n",
       " 36,\n",
       " 3,\n",
       " 6,\n",
       " 31,\n",
       " 6,\n",
       " 3,\n",
       " 19,\n",
       " 24,\n",
       " 6,\n",
       " 3,\n",
       " 30,\n",
       " 25,\n",
       " 3,\n",
       " 16,\n",
       " 3,\n",
       " 24,\n",
       " 3,\n",
       " 16,\n",
       " 25,\n",
       " 13,\n",
       " 27,\n",
       " 33,\n",
       " 3,\n",
       " 28,\n",
       " 16,\n",
       " 5,\n",
       " 3,\n",
       " 36,\n",
       " 30,\n",
       " 27,\n",
       " 33,\n",
       " 17,\n",
       " 6,\n",
       " 16,\n",
       " 27,\n",
       " 33,\n",
       " 5,\n",
       " 32,\n",
       " 24,\n",
       " 13,\n",
       " 6,\n",
       " 3,\n",
       " 28,\n",
       " 3,\n",
       " 16,\n",
       " 6,\n",
       " 32,\n",
       " 32,\n",
       " 3,\n",
       " 30,\n",
       " 19,\n",
       " 13,\n",
       " 5,\n",
       " 3,\n",
       " 16,\n",
       " 17,\n",
       " 24,\n",
       " 6,\n",
       " 16,\n",
       " 13,\n",
       " 3,\n",
       " 17,\n",
       " 12,\n",
       " 13,\n",
       " 24,\n",
       " 29,\n",
       " 6,\n",
       " 3,\n",
       " 27,\n",
       " 33,\n",
       " 17,\n",
       " 3,\n",
       " 3,\n",
       " 25,\n",
       " 32,\n",
       " 13,\n",
       " 36,\n",
       " 6,\n",
       " 6,\n",
       " 22,\n",
       " 3,\n",
       " 24,\n",
       " 25,\n",
       " 6,\n",
       " 32,\n",
       " 22,\n",
       " 32,\n",
       " 3,\n",
       " 16,\n",
       " 6,\n",
       " 22,\n",
       " 27,\n",
       " 33,\n",
       " 35,\n",
       " 29,\n",
       " 3,\n",
       " 31,\n",
       " 19,\n",
       " 24,\n",
       " 32,\n",
       " 32,\n",
       " 3,\n",
       " 13,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 6,\n",
       " 32,\n",
       " 29,\n",
       " 24,\n",
       " 6,\n",
       " 27,\n",
       " 33,\n",
       " 33,\n",
       " 35,\n",
       " 12,\n",
       " 30,\n",
       " 32,\n",
       " 5,\n",
       " 13,\n",
       " 32,\n",
       " 25,\n",
       " 35,\n",
       " 27,\n",
       " 33,\n",
       " 3,\n",
       " 30,\n",
       " 19,\n",
       " 14,\n",
       " 12,\n",
       " 17,\n",
       " 32,\n",
       " 19,\n",
       " 32,\n",
       " 24,\n",
       " 6,\n",
       " 3,\n",
       " 32,\n",
       " 3,\n",
       " 31,\n",
       " 17,\n",
       " 6,\n",
       " 13,\n",
       " 32,\n",
       " 19,\n",
       " 29,\n",
       " 25,\n",
       " 24,\n",
       " 13,\n",
       " 24,\n",
       " 22,\n",
       " 31,\n",
       " 23,\n",
       " 17,\n",
       " 5,\n",
       " 13,\n",
       " 3,\n",
       " 32,\n",
       " 22,\n",
       " 32,\n",
       " 36,\n",
       " 32,\n",
       " 3,\n",
       " 3,\n",
       " 25,\n",
       " 24,\n",
       " 27,\n",
       " 33,\n",
       " 27,\n",
       " 33,\n",
       " 24,\n",
       " 6,\n",
       " 32,\n",
       " 24,\n",
       " 19,\n",
       " 13,\n",
       " 19,\n",
       " 3,\n",
       " 24,\n",
       " 5,\n",
       " 36,\n",
       " 12,\n",
       " 14,\n",
       " 27,\n",
       " 33,\n",
       " 30,\n",
       " 27,\n",
       " 33,\n",
       " 13,\n",
       " 19,\n",
       " 24,\n",
       " 3,\n",
       " 33,\n",
       " 18,\n",
       " 12,\n",
       " 3,\n",
       " 24,\n",
       " 6,\n",
       " 32,\n",
       " 30,\n",
       " 16,\n",
       " 13,\n",
       " 19,\n",
       " 31,\n",
       " 31,\n",
       " 24,\n",
       " 32,\n",
       " 17,\n",
       " 25,\n",
       " 13,\n",
       " 24,\n",
       " 32,\n",
       " 32,\n",
       " 20,\n",
       " 14,\n",
       " 3,\n",
       " 17,\n",
       " 29,\n",
       " 19,\n",
       " 32,\n",
       " 24,\n",
       " 13,\n",
       " 17,\n",
       " 32,\n",
       " 3,\n",
       " 17,\n",
       " 5,\n",
       " 24,\n",
       " 3,\n",
       " 19,\n",
       " 3,\n",
       " 32,\n",
       " 24,\n",
       " 5,\n",
       " 32,\n",
       " 32,\n",
       " 27,\n",
       " 33,\n",
       " 27,\n",
       " 33,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 31,\n",
       " 32,\n",
       " 3,\n",
       " 25,\n",
       " 32,\n",
       " 6,\n",
       " 32,\n",
       " 19,\n",
       " 19,\n",
       " 12,\n",
       " 32,\n",
       " 29,\n",
       " 5,\n",
       " 28,\n",
       " 29,\n",
       " 36,\n",
       " 36,\n",
       " 31,\n",
       " 19,\n",
       " 3,\n",
       " 33,\n",
       " 25,\n",
       " 3,\n",
       " 5,\n",
       " 32,\n",
       " 6,\n",
       " 32,\n",
       " 27,\n",
       " 33,\n",
       " 31,\n",
       " 19,\n",
       " 3,\n",
       " 25,\n",
       " 6,\n",
       " 6,\n",
       " 16,\n",
       " 12,\n",
       " 5,\n",
       " 16,\n",
       " 14,\n",
       " 22,\n",
       " 28,\n",
       " 6,\n",
       " 29,\n",
       " 6,\n",
       " 32,\n",
       " 29,\n",
       " 24,\n",
       " 6,\n",
       " 22,\n",
       " 33,\n",
       " 5,\n",
       " 3,\n",
       " 36,\n",
       " 25,\n",
       " 13,\n",
       " 3,\n",
       " 5,\n",
       " 24,\n",
       " 29,\n",
       " 5,\n",
       " 16,\n",
       " 5,\n",
       " 10,\n",
       " 32,\n",
       " 24,\n",
       " 18,\n",
       " 33,\n",
       " 31,\n",
       " 33,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 16,\n",
       " 32,\n",
       " 14,\n",
       " 19,\n",
       " 32,\n",
       " 24,\n",
       " 3,\n",
       " 25,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 22,\n",
       " 3,\n",
       " 24,\n",
       " 3,\n",
       " 24,\n",
       " 12,\n",
       " 32,\n",
       " 25,\n",
       " 24,\n",
       " 35,\n",
       " 25,\n",
       " 16,\n",
       " 16,\n",
       " 25,\n",
       " 13,\n",
       " 17,\n",
       " 6,\n",
       " 12,\n",
       " 3,\n",
       " 3,\n",
       " 32,\n",
       " 5,\n",
       " 5,\n",
       " 31,\n",
       " 19,\n",
       " 30,\n",
       " 19,\n",
       " 10,\n",
       " 3,\n",
       " 25,\n",
       " 6,\n",
       " 32,\n",
       " 19,\n",
       " 30,\n",
       " 28,\n",
       " 29,\n",
       " 10,\n",
       " 5,\n",
       " 6,\n",
       " 32,\n",
       " 32,\n",
       " 29,\n",
       " 24,\n",
       " 6,\n",
       " 32,\n",
       " 19,\n",
       " 10,\n",
       " 3,\n",
       " 25,\n",
       " 35,\n",
       " 29,\n",
       " 27,\n",
       " 33,\n",
       " 36,\n",
       " 17,\n",
       " 6,\n",
       " 3,\n",
       " 32,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 17,\n",
       " 31,\n",
       " 3,\n",
       " 6,\n",
       " 13,\n",
       " 16,\n",
       " 32,\n",
       " 24,\n",
       " 3,\n",
       " 32,\n",
       " 30,\n",
       " 13,\n",
       " 32,\n",
       " 29,\n",
       " 17,\n",
       " 13,\n",
       " 24,\n",
       " 13,\n",
       " 3,\n",
       " 6,\n",
       " 13,\n",
       " 3,\n",
       " 36,\n",
       " 16,\n",
       " 25,\n",
       " 24,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 24,\n",
       " 12,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 16,\n",
       " 24,\n",
       " 30,\n",
       " 16,\n",
       " 36,\n",
       " 5,\n",
       " 6,\n",
       " 16,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 13,\n",
       " 6,\n",
       " 29,\n",
       " 32,\n",
       " 24,\n",
       " 24,\n",
       " 17,\n",
       " 6,\n",
       " 19,\n",
       " 16,\n",
       " 35,\n",
       " 35,\n",
       " 6,\n",
       " 32,\n",
       " 24,\n",
       " 24,\n",
       " 14,\n",
       " 28,\n",
       " 6,\n",
       " 6,\n",
       " 32,\n",
       " 13,\n",
       " 3,\n",
       " 32,\n",
       " 17,\n",
       " 27,\n",
       " 33,\n",
       " 24,\n",
       " 28,\n",
       " 17,\n",
       " 19,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 16,\n",
       " 32,\n",
       " 27,\n",
       " 33,\n",
       " 19,\n",
       " 6,\n",
       " 29,\n",
       " 17,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 27,\n",
       " 33,\n",
       " 24,\n",
       " 13,\n",
       " 10,\n",
       " 6,\n",
       " 22,\n",
       " 32,\n",
       " 12,\n",
       " 17,\n",
       " 36,\n",
       " 24,\n",
       " 22,\n",
       " 24,\n",
       " 32,\n",
       " 24,\n",
       " 32,\n",
       " 34,\n",
       " 3,\n",
       " 17,\n",
       " 6,\n",
       " 32,\n",
       " 16,\n",
       " 22,\n",
       " 17,\n",
       " 29,\n",
       " 24,\n",
       " 0,\n",
       " 19,\n",
       " 13,\n",
       " 24,\n",
       " 6,\n",
       " 25,\n",
       " 32,\n",
       " 19,\n",
       " 24,\n",
       " 6,\n",
       " 23,\n",
       " 3,\n",
       " 17,\n",
       " 19,\n",
       " 3,\n",
       " 16,\n",
       " 15,\n",
       " 24,\n",
       " 32,\n",
       " 12,\n",
       " 6,\n",
       " 22,\n",
       " 24,\n",
       " 31,\n",
       " 32,\n",
       " 3,\n",
       " 28,\n",
       " 16,\n",
       " 5,\n",
       " 3,\n",
       " 24,\n",
       " 28,\n",
       " 25,\n",
       " 25,\n",
       " 3,\n",
       " 28,\n",
       " 5,\n",
       " 16,\n",
       " 3,\n",
       " 3,\n",
       " 17,\n",
       " 6,\n",
       " 3,\n",
       " 25,\n",
       " 27,\n",
       " 33,\n",
       " 19,\n",
       " 12,\n",
       " 34,\n",
       " 15,\n",
       " 29,\n",
       " 19,\n",
       " 27,\n",
       " 33,\n",
       " 36,\n",
       " 3,\n",
       " 3,\n",
       " 24,\n",
       " 13,\n",
       " 27,\n",
       " 33,\n",
       " 32,\n",
       " 24,\n",
       " 31,\n",
       " 5,\n",
       " 28,\n",
       " 30,\n",
       " 31,\n",
       " 25,\n",
       " 16,\n",
       " 31,\n",
       " 12,\n",
       " 36,\n",
       " 6,\n",
       " 29,\n",
       " 35,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 28,\n",
       " 28,\n",
       " 24,\n",
       " 10,\n",
       " 27,\n",
       " 33,\n",
       " 31,\n",
       " 13,\n",
       " 29,\n",
       " 35,\n",
       " 24,\n",
       " 22,\n",
       " 24,\n",
       " 5,\n",
       " 27,\n",
       " 33,\n",
       " 32,\n",
       " 6,\n",
       " 13,\n",
       " 32,\n",
       " 17,\n",
       " 6,\n",
       " 32,\n",
       " 6,\n",
       " 3,\n",
       " 16,\n",
       " 6,\n",
       " 19,\n",
       " 25,\n",
       " 36,\n",
       " 5,\n",
       " 28,\n",
       " 32,\n",
       " 25,\n",
       " 3,\n",
       " 27,\n",
       " 33,\n",
       " 12,\n",
       " 32,\n",
       " 19,\n",
       " 36,\n",
       " 3,\n",
       " 32,\n",
       " 24,\n",
       " 28,\n",
       " 17,\n",
       " 30,\n",
       " 24,\n",
       " 19,\n",
       " 10,\n",
       " 24,\n",
       " 3,\n",
       " 13,\n",
       " 16,\n",
       " 23,\n",
       " 17,\n",
       " 6,\n",
       " 16,\n",
       " 13,\n",
       " 3,\n",
       " 3,\n",
       " 27,\n",
       " 33,\n",
       " 19,\n",
       " 5,\n",
       " 24,\n",
       " 3,\n",
       " 24,\n",
       " 3,\n",
       " 25,\n",
       " 32,\n",
       " 13,\n",
       " 10,\n",
       " 34,\n",
       " 32,\n",
       " 17,\n",
       " 31,\n",
       " 6,\n",
       " 24,\n",
       " 17,\n",
       " 24,\n",
       " 32,\n",
       " 24,\n",
       " 3,\n",
       " 17,\n",
       " 6,\n",
       " 16,\n",
       " 30,\n",
       " 3,\n",
       " 19,\n",
       " 23,\n",
       " 24,\n",
       " 32,\n",
       " 22,\n",
       " 19,\n",
       " 13,\n",
       " 6,\n",
       " 25,\n",
       " 24,\n",
       " 3,\n",
       " 19,\n",
       " 12,\n",
       " 33,\n",
       " 34,\n",
       " 19,\n",
       " 13,\n",
       " 12,\n",
       " 30,\n",
       " 12,\n",
       " 24,\n",
       " 24,\n",
       " 19,\n",
       " 10,\n",
       " 28,\n",
       " 25,\n",
       " 3,\n",
       " 31,\n",
       " 17,\n",
       " 13,\n",
       " 6,\n",
       " 6,\n",
       " 27,\n",
       " 33,\n",
       " 13,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "dataset['train'][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}