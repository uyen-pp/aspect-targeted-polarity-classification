{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('python37': conda)",
   "metadata": {
    "interpreter": {
     "hash": "da2a2dd739486b98df65bba8b6599fd6aa55ff0b355cb46f8657e6daa711dc7a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This notebook is for testing of:\n",
    "- Data transformation from a \"cleaned, preprocessed, splited\" pickle file \n",
    "- Model loading\n",
    "- Model training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(PROJECT_DIR)\n",
    "sys.path.append(os.path.join(PROJECT_DIR, \"finetuning_and_classification\"))\n",
    "\n",
    "\n",
    "import utils\n",
    "from utils_glue import (compute_metrics, convert_examples_to_features,\n",
    "                        output_modes, processors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data load from pkl\n",
    "# Test function YNmilk_data_reform\n",
    "\n",
    "sentences, aspect_term_sentiments =utils.YNmilk_data_reform(\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\data_splited\\\\train.pickle\")\n",
    "\n",
    "sentences[0:5], aspect_term_sentiments[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "!python prepare_YNmilk_datasets.py \\\n",
    "    --files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\train.pickle\" \\\n",
    "    --output_dir data/transformed/yndata \\\n",
    "    --istrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepare_YNmilk_datasets.py \\\n",
    "--files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\validate.pickle\" \\\n",
    "--output_dir data/transformed/yndata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "MODEL_DIR = os.path.join(PROJECT_DIR, \"data/models/phobert_base/huggingface_phobert_base\")\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"data/transformed/yndata\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_DIR, \"YNData-ar-phobert-milk\")\n",
    "\n",
    "!python ./finetuning_and_classification/run_glue.py \\\n",
    "    --model_type=\"phobert\" \\\n",
    "    --model_name_or_path=C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base \\\n",
    "    --do_train \\\n",
    "    # --evaluate_during_training \\\n",
    "    --logging_steps 100 --save_steps 200 --task_name=\"atsc\" \\\n",
    "    --seed 42 \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir=\"$DATA_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --max_seq_length=128 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --per_gpu_eval_batch_size=32 \\\n",
    "    --gradient_accumulation_steps=1 \\\n",
    "    --max_steps=800 \\\n",
    "    --overwrite_output_dir --overwrite_cache --warmup_steps=120\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "\n",
    "\n",
    "# config_class, model_class, tokenizer_class = (BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "# config = config_class.from_pretrained(MODEL_DIR, num_labels=num_labels, finetuning_task=task_name)\n",
    "# tokenizer = tokenizer_class.from_pretrained(MODEL_DIR, do_lower_case=True)\n",
    "# model = model_class.from_pretrained(MODEL_DIR, from_tf=bool('.ckpt' in MODEL_DIR), config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepare_YNmilk_datasets.py \\\n",
    "--files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\test.pickle\" \\\n",
    "--output_dir data/transformed/yndata/testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "!python ./finetuning_and_classification/run_glue.py \\\n",
    "    --model_type=\"phobert\" \\\n",
    "    --do_eval \\\n",
    "    --model_name_or_path=\"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification/data/models/finetuned_phobert4classification/\" \\\n",
    "    --task_name=\"atsc\" \\\n",
    "    --output_dir=\"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification/data/models/finetuned_phobert4classification/\"  \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir=./data/transformed/yndata/testdata \\\n",
    "    --max_seq_length=128 \\\n",
    "    --per_gpu_eval_batch_size=32 \\\n",
    "    --overwrite_output_dir --overwrite_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text parser\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "filepath = \"C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/transformed/yndata/testdata/test.xml\"\n",
    "\n",
    "with open(filepath, 'rb') as f:\n",
    "    sentence_elements = ET.parse(f).getroot().iter('sentence')\n",
    "\n",
    "e_0 = list(sentence_elements)[0]\n",
    "e_0.find('text').text, e_0.find('aspectTerms').find('aspectTerm').get('term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-0365d4a76ec2245b\n",
      "Downloading and preparing dataset ar_dataset/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\Uyen\\.cache\\huggingface\\datasets\\ar_dataset\\default-0365d4a76ec2245b\\0.0.0\\ac6d68984bc8ec6167fa960dba1b07bd060b01dd4d3115652428832d6a179e17...\n",
      "                                ['Quà tặng']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False]\n",
      "['Chức_năng tiêu_hóa', 'Nguồn_gốc xuất_xứ', 'Hấp_thụ', 'Dinh_dưỡng']\n",
      "[False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Thành_phần']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False]\n",
      "['Phát_triển trí_não']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "['Nóng', 'Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chất_lượng chung']\n",
      "[False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Phân_phối']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False]\n",
      "['Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chất_lượng chung']\n",
      "[False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Hương_vị', 'Nhãn_hiệu']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chức_năng tiêu_hóa']\n",
      "[False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chức_năng tiêu_hóa']\n",
      "[False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Quà tặng']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False]\n",
      "['Phát_triển thể_chất', 'Nhãn_hiệu']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False]\n",
      "['Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Dataset ar_dataset downloaded and prepared to C:\\Users\\Uyen\\.cache\\huggingface\\datasets\\ar_dataset\\default-0365d4a76ec2245b\\0.0.0\\ac6d68984bc8ec6167fa960dba1b07bd060b01dd4d3115652428832d6a179e17. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader\n",
    "# !pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "data_dir=\"C:/Users/Uyen/Documents/nlp/YNdata/data_test\"\n",
    "train_file = os.path.join(data_dir, \"train.csv\")\n",
    "eval_file = os.path.join(data_dir, \"dev.csv\")\n",
    "\n",
    "dataset = load_dataset('C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\thesis\\\\aspect-targeted-polarity-classification\\\\finetuning_and_classification\\\\load_dataset_ar.py', data_files={\"train\": train_file, \"validation\": eval_file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[31, 3, 32, 29, 25, 2, 30, 6]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dataset[\"train\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# YN Data proposed\n",
    "YN_DATA_HOME = \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\"\n",
    "\n",
    "# Clean YN Data dataframe pickle files\n",
    "YN_CLEAN = os.path.join(YN_DATA_HOME, \"splitted\")\n",
    "\n",
    "# YN Data transfrom to xml to be run_glue adaptable, for training\n",
    "YN_AR = os.path.join(YN_DATA_HOME, \"ar_dataset\")\n",
    "\n",
    "YN_ATSC = os.path.join(YN_DATA_HOME, \"atsc_dataset\")\n",
    "\n",
    "# YN Data transfrom to xml to be run_glue adaptable, for testing\n",
    "YN_TEST = os.path.join(YN_DATA_HOME, \"test\")\n",
    "\n",
    "# Aspect regconition model\n",
    "ASPECT_RECOGNITION_MODEL_DIR = \"./AR\"\n",
    "\n",
    "# Target-based sentiment classification model\n",
    "TARGETED_POLARITY_CLASSIFICATION_MODEL_DIR = \"/gdrive/MyDrive/Thesis/Models/TASC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = os.path.join(YN_AR, 'train.csv')\n",
    "EVAL_FILE = os.path.join(YN_AR, 'dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "03/15/2021 11:18:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "\n",
      "0 examples [00:00, ? examples/s]\n",
      "363 examples [00:00, 3616.60 examples/s]\n",
      "818 examples [00:00, 3853.71 examples/s]\n",
      "1286 examples [00:00, 4058.68 examples/s]\n",
      "1690 examples [00:00, 4040.89 examples/s]\n",
      "2132 examples [00:00, 4135.96 examples/s]\n",
      "2581 examples [00:00, 4236.15 examples/s]\n",
      "3057 examples [00:00, 4368.82 examples/s]\n",
      "3522 examples [00:00, 4449.44 examples/s]\n",
      "3960 examples [00:00, 4415.03 examples/s]\n",
      "4469 examples [00:01, 4597.53 examples/s]\n",
      "4974 examples [00:01, 4711.29 examples/s]\n",
      "5484 examples [00:01, 4808.37 examples/s]\n",
      "6006 examples [00:01, 4910.52 examples/s]\n",
      "6530 examples [00:01, 5004.92 examples/s]\n",
      "7059 examples [00:01, 5072.56 examples/s]\n",
      "7567 examples [00:01, 5059.57 examples/s]\n",
      "8073 examples [00:01, 5059.56 examples/s]03/15/2021 11:18:55 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./AR_Model, overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs\\Mar15_11-18-55_UyenPc, logging_strategy=LoggingStrategy.STEPS, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./AR_Model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
      "03/15/2021 11:18:55 - INFO - __main__ -   *** Load data for task ar\n",
      "03/15/2021 11:18:55 - INFO - __main__ -   load a local file for train: C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\ar_dataset\\\\train.csv\n",
      "03/15/2021 11:18:55 - INFO - __main__ -   load a local file for validation: C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\ar_dataset\\\\dev.csv\n",
      "03/15/2021 11:18:55 - INFO - __main__ -   Load data for task ar\n",
      "03/15/2021 11:18:57 - WARNING - datasets.load -   Using the latest cached version of the module from C:\\Users\\Uyen\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\load_dataset_ar\\ac6d68984bc8ec6167fa960dba1b07bd060b01dd4d3115652428832d6a179e17 (last modified on Mon Mar 15 01:09:30 2021) since it couldn't be found locally at ./load_dataset_ar.py\\load_dataset_ar.py or remotely (FileNotFoundError).\n",
      "03/15/2021 11:18:57 - WARNING - datasets.builder -   Using custom data configuration default-1b949a7dcd9353c8\n",
      "Downloading and preparing dataset ar_dataset/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\Uyen\\.cache\\huggingface\\datasets\\ar_dataset\\default-1b949a7dcd9353c8\\0.0.0\\ac6d68984bc8ec6167fa960dba1b07bd060b01dd4d3115652428832d6a179e17...\n",
      "['Quà tặng']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False]\n",
      "['Chức_năng tiêu_hóa', 'Nguồn_gốc xuất_xứ', 'Hấp_thụ', 'Dinh_dưỡng']\n",
      "[False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Thành_phần']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False]\n",
      "['Phát_triển trí_não']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "['Nóng', 'Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chất_lượng chung']\n",
      "[False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Phân_phối']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False]\n",
      "['Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Khuyến_mại']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Thương_mại_điện_tử', 'Nội_dung Tiếp_thị điện_tử']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False]\n",
      "['Phát_triển thể_chất']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False]\n",
      "['Phát_triển thể_chất', 'Hấp_thụ', 'Chất_lượng chung', 'Nhãn_hiệu']\n",
      "[False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False]\n",
      "['Cân nặng']\n",
      "[False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Thành_phần', 'Hương_vị']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False]\n",
      "['Tiện_lợi']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False]\n",
      "['Nóng', 'Nhãn_hiệu']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chức_năng tiêu_hóa']\n",
      "[False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "8579 examples [00:01, 5000.42 examples/s]\n",
      "9080 examples [00:01, 4273.51 examples/s]\n",
      "9542 examples [00:02, 4371.86 examples/s]\n",
      "10000 examples [00:02, 3756.18 examples/s]\n",
      "10466 examples [00:02, 3988.21 examples/s]\n",
      "10935 examples [00:02, 4164.56 examples/s]\n",
      "11395 examples [00:02, 4286.28 examples/s]\n",
      "11847 examples [00:02, 4341.28 examples/s]\n",
      "12291 examples [00:02, 4306.79 examples/s]\n",
      "12754 examples [00:02, 4386.47 examples/s]\n",
      "13198 examples [00:02, 4363.43 examples/s]\n",
      "13657 examples [00:03, 4416.25 examples/s]\n",
      "14128 examples [00:03, 4500.45 examples/s]\n",
      "14586 examples [00:03, 4510.67 examples/s]\n",
      "15039 examples [00:03, 4463.03 examples/s]\n",
      "15509 examples [00:03, 4531.57 examples/s]\n",
      "15972 examples [00:03, 4560.66 examples/s]\n",
      "16458 examples [00:03, 4633.23 examples/s]\n",
      "16923 examples [00:03, 4624.41 examples/s]\n",
      "17416 examples [00:03, 4712.03 examples/s]\n",
      "17889 examples [00:03, 4717.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "!python ./finetuning_and_classification/my_run.py      --model_name_or_path=\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\thesis\\\\aspect-targeted-polarity-classification\\\\data\\\\models\\\\phobert_base\\\\huggingface_phobert_base\"     --do_train     --use_fast_tokenizer=False     --task_name=\"ar\"      --max_seq_length=\"256\"     --overwrite_cache=True     --pad_to_max_length=True     --train_file=\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\ar_dataset\\\\train.csv\"      --validation_file=\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\ar_dataset\\\\dev.csv\"      --output_dir=\"./AR_Model\" --overwrite_output_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}