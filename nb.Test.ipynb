{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('python37': conda)",
   "metadata": {
    "interpreter": {
     "hash": "da2a2dd739486b98df65bba8b6599fd6aa55ff0b355cb46f8657e6daa711dc7a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This notebook is for testing of:\n",
    "- Data transformation from a \"cleaned, preprocessed, splited\" pickle file \n",
    "- Model loading\n",
    "- Model training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(PROJECT_DIR)\n",
    "sys.path.append(os.path.join(PROJECT_DIR, \"finetuning_and_classification\"))\n",
    "\n",
    "\n",
    "import utils\n",
    "from utils_glue import (compute_metrics, convert_examples_to_features,\n",
    "                        output_modes, processors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([\"'ng√†y u·ªëng ƒë∆∞·ª£c 3 c·ªØ th√¨ t·ªët nh·∫•t ch·ªã ·∫°'\",\n",
       "  \"'Th∆°m ngon b·ªï d∆∞·ª°ng'\",\n",
       "  \"'Th∆°m ngon b·ªï d∆∞·ª°ng'\",\n",
       "  \"'Minh Anh LeƒëuÃõÃÄng queÃÇn gheÃÅ ngay cuÃõÃâa haÃÄng GiaÃÇÃÅc moÃõ suÃõÃÉa VieÃ£ÃÇt ƒëeÃÇÃâ mua ngay suÃõÃÉa Yoko ƒëeÃÇÃâ rinh nhuÃõÃÉng moÃÅn quaÃÄ ƒëaÃÅng yeÃÇu cho beÃÅ nhaüòÑ'\",\n",
       "  \"'Hay qu√°'\"],\n",
       " [[('Generic', 'POS')],\n",
       "  [('Taste', 'POS')],\n",
       "  [('Nutrition', 'POS')],\n",
       "  [('Gift', 'POS')],\n",
       "  [('Online', 'POS')]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Test data load from pkl\n",
    "sentences, aspect_term_sentiments =utils.YNmilk_data_reform(\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\data_splited\\\\train.pickle\")\n",
    "\n",
    "sentences[0:5], aspect_term_sentiments[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/transformed/yndata\nC:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\train.pickle\nDataset: Train\n#Sentences with minimum 1 label 178065\nLabel Counts [('POS', 145849), ('NEU', 29108), ('NEG', 3108)]\n#SentencePairs 178065\nPOS% 0.8190773032319658\nNEG% 0.01745430039592284\nNEU% 0.1634683963721113\nPOS/NEG 46.926962676962674\nPOS/NEU 5.010615638312491\nNEG/NEU 0.10677476982272914\n\nDataset: Dev\n#Sentences with minimum 1 label 17806\nLabel Counts [('POS', 14681), ('NEU', 2825), ('NEG', 300)]\n#SentencePairs 17806\nPOS% 0.824497360440301\nNEG% 0.016848253397731102\nNEU% 0.15865438616196786\nPOS/NEG 48.93666666666667\nPOS/NEU 5.196814159292035\nNEG/NEU 0.10619469026548672\n\nDataset: TrainSplit\n#Sentences with minimum 1 label 160259\nLabel Counts [('POS', 131168), ('NEU', 26283), ('NEG', 2808)]\n#SentencePairs 160259\nPOS% 0.8184750934424899\nNEG% 0.0175216368503485\nNEU% 0.16400326970716153\nPOS/NEG 46.712250712250714\nPOS/NEU 4.990602290453905\nNEG/NEU 0.10683711905033672\n\n"
     ]
    }
   ],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "!python prepare_YNmilk_datasets.py \\\n",
    "    --files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\train.pickle\" \\\n",
    "    --output_dir data/transformed/yndata \\\n",
    "    --istrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/transformed/yndata\nC:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\validate.pickle\nDataset: Test\n#Sentences with minimum 1 label 38427\nLabel Counts [('POS', 31555), ('NEU', 6166), ('NEG', 706)]\n#SentencePairs 38427\nPOS% 0.8211674083326828\nNEG% 0.018372498503656284\nNEU% 0.16046009316366097\nPOS/NEG 44.695467422096314\nPOS/NEU 5.117580278949076\nNEG/NEU 0.11449886474213429\n\n"
     ]
    }
   ],
   "source": [
    "!python prepare_YNmilk_datasets.py \\\n",
    "--files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\validate.pickle\" \\\n",
    "--output_dir data/transformed/yndata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ParseError",
     "evalue": "no element found: line 8, column 13 (<string>)",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3427\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-11-28f6b46ff6bb>\"\u001b[0m, line \u001b[0;32m4\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    sentence_elements = ET.parse(file).getroot().iter('sentence')\n",
      "  File \u001b[0;32m\"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\xml\\etree\\ElementTree.py\"\u001b[0m, line \u001b[0;32m1197\u001b[0m, in \u001b[0;35mparse\u001b[0m\n    tree.parse(source, parser)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\xml\\etree\\ElementTree.py\"\u001b[1;36m, line \u001b[1;32m598\u001b[1;36m, in \u001b[1;35mparse\u001b[1;36m\u001b[0m\n\u001b[1;33m    self._root = parser._parse_whole(source)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32munknown\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m no element found: line 8, column 13\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "with open(\"C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/transformed/yndata/testtest.xml\", \"r\", encoding=\"utf-8\") as file:\n",
    "    sentence_elements = ET.parse(file).getroot().iter('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "02/25/2021 20:57:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "02/25/2021 20:57:45 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\\\\data/transformed/yndata', device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=3e-05, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=128, max_steps=800, model_name_or_path='C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base', model_type='phobert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\\\\YNData-atsc-phobert-milk', output_mode='classification', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=8, save_steps=200, seed=42, server_ip='', server_port='', task_name='atsc', tokenizer_name='', warmup_steps=120, weight_decay=0.0)\n",
      "02/25/2021 20:57:45 - INFO - __main__ -   Creating features from dataset file at C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\\data/transformed/yndata\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   Writing example 0 of 178065\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   *** Example ***\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   guid: train-0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   tokens: <s> '@@ ng√†y u·ªëng ƒë∆∞·ª£c 3 c·ªØ th√¨ t·ªët nh·∫•t ch·ªã ·∫°@@ ' </s> Gener@@ ic </s>\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_ids: 0 51899 43 653 11 107 35417 54 167 67 213 13628 104 2 27707 4933 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   label: POS (id = 0)\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   *** Example ***\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   guid: train-1\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   tokens: <s> '@@ Th∆°m ngon b·ªï d∆∞·ª°@@ ng@@ ' </s> Nutri@@ tion </s>\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_ids: 0 51899 10474 1325 5665 55168 1701 104 2 39881 6107 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   label: POS (id = 0)\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   *** Example ***\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   guid: train-2\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   tokens: <s> '@@ Th∆°m ngon b·ªï d∆∞·ª°@@ ng@@ ' </s> Tas@@ te </s>\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_ids: 0 51899 10474 1325 5665 55168 1701 104 2 17163 3915 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   label: POS (id = 0)\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   *** Example ***\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   guid: train-3\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   tokens: <s> '@@ Minh Anh Le@@ ƒëu@@ Ãõ@@ ÃÄ@@ ng que@@ ÃÇ@@ n ghe@@ ÃÅ ngay cu@@ Ãõ@@ Ãâ@@ a ha@@ ÃÄ@@ ng Gia@@ ÃÇ@@ ÃÅ@@ c mo@@ Ãõ su@@ Ãõ@@ ÃÉ@@ a Vie@@ Ã£@@ ÃÇ@@ t ƒëe@@ ÃÇ@@ Ãâ mua ngay su@@ Ãõ@@ ÃÉ@@ a Yo@@ ko ƒëe@@ ÃÇ@@ Ãâ rinh nhu@@ Ãõ@@ ÃÉ@@ ng mo@@ ÃÅ@@ n qua@@ ÃÄ ƒëa@@ ÃÅ@@ ng ye@@ ÃÇ@@ u cho be@@ ÃÅ nha@@ üòÑ@@ ' </s> Gi@@ ft </s>\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_ids: 0 51899 1612 157 2923 26728 3 58216 2795 27262 3 1677 37660 60455 249 5109 3 60198 1517 8216 58216 2795 34426 3 45729 1894 3773 3 3611 3 61685 1517 35559 55479 3 1204 34463 3 61152 188 249 3611 3 61685 1517 9674 5516 34463 3 61152 25324 24913 3 61685 2795 3773 45729 1677 38937 61538 20327 45729 2795 10081 3 1656 13 5329 60455 37361 3 104 2 2946 6628 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   label: POS (id = 0)\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   *** Example ***\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   guid: train-4\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   tokens: <s> '@@ Hay qu√°@@ ' </s> Con@@ t@@ ent@@ Qual@@ ity </s>\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_ids: 0 51899 3896 20027 104 2 4786 1187 6502 55447 8233 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/25/2021 20:57:50 - INFO - utils_glue -   label: POS (id = 0)\n",
      "02/25/2021 20:57:53 - INFO - utils_glue -   Writing example 10000 of 178065\n",
      "02/25/2021 20:57:55 - INFO - utils_glue -   Writing example 20000 of 178065\n",
      "02/25/2021 20:57:57 - INFO - utils_glue -   Writing example 30000 of 178065\n",
      "02/25/2021 20:58:00 - INFO - utils_glue -   Writing example 40000 of 178065\n",
      "02/25/2021 20:58:02 - INFO - utils_glue -   Writing example 50000 of 178065\n",
      "02/25/2021 20:58:04 - INFO - utils_glue -   Writing example 60000 of 178065\n",
      "02/25/2021 20:58:07 - INFO - utils_glue -   Writing example 70000 of 178065\n",
      "02/25/2021 20:58:09 - INFO - utils_glue -   Writing example 80000 of 178065\n",
      "02/25/2021 20:58:11 - INFO - utils_glue -   Writing example 90000 of 178065\n",
      "02/25/2021 20:58:14 - INFO - utils_glue -   Writing example 100000 of 178065\n",
      "02/25/2021 20:58:16 - INFO - utils_glue -   Writing example 110000 of 178065\n",
      "02/25/2021 20:58:18 - INFO - utils_glue -   Writing example 120000 of 178065\n",
      "02/25/2021 20:58:21 - INFO - utils_glue -   Writing example 130000 of 178065\n",
      "02/25/2021 20:58:23 - INFO - utils_glue -   Writing example 140000 of 178065\n",
      "02/25/2021 20:58:25 - INFO - utils_glue -   Writing example 150000 of 178065\n",
      "02/25/2021 20:58:27 - INFO - utils_glue -   Writing example 160000 of 178065\n",
      "02/25/2021 20:58:30 - INFO - utils_glue -   Writing example 170000 of 178065\n",
      "02/25/2021 20:58:32 - INFO - __main__ -   Saving features into cached file C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\\data/transformed/yndata\\cached_train_huggingface_phobert_base_128_atsc\n",
      "02/25/2021 20:59:09 - INFO - __main__ -   ***** Running training *****\n",
      "02/25/2021 20:59:09 - INFO - __main__ -     Num examples = 178065\n",
      "02/25/2021 20:59:09 - INFO - __main__ -     Num Epochs = 1\n",
      "02/25/2021 20:59:09 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n",
      "02/25/2021 20:59:09 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "02/25/2021 20:59:09 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "02/25/2021 20:59:09 - INFO - __main__ -     Total optimization steps = 800\n",
      "\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Iteration:   0%|          | 0/22259 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Iteration:   0%|          | 1/22259 [00:01<10:52:48,  1.76s/it]\u001b[A\n",
      "Iteration:   0%|          | 1/22259 [00:02<13:46:48,  2.23s/it]\n",
      "\n",
      "Epoch:   0%|          | 0/1 [00:02<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"./finetuning_and_classification/run_glue.py\", line 509, in <module>\n",
      "    main()\n",
      "  File \"./finetuning_and_classification/run_glue.py\", line 463, in main\n",
      "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
      "  File \"./finetuning_and_classification/run_glue.py\", line 121, in train\n",
      "    ouputs = model(**inputs)\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 1155, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 817, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 514, in forward\n",
      "    output_attentions,\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 399, in forward\n",
      "    past_key_value=self_attn_past_key_value,\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 329, in forward\n",
      "    output_attentions,\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Uyen\\anaconda3\\envs\\python37\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 258, in forward\n",
      "    context_layer = torch.matmul(attention_probs, value_layer)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.74 GiB already allocated; 17.23 MiB free; 2.90 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "MODEL_DIR = os.path.join(PROJECT_DIR, \"data/models/phobert_base/huggingface_phobert_base\")\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"data/transformed/yndata\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_DIR, \"YNData-atsc-phobert-milk\")\n",
    "\n",
    "!python ./finetuning_and_classification/run_glue.py \\\n",
    "    --model_type=\"phobert\" \\\n",
    "    --model_name_or_path=C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base \\\n",
    "    --do_train \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 100 --save_steps 200 --task_name=\"atsc\" \\\n",
    "    --seed 42 \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir=\"$DATA_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --max_seq_length=128 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --per_gpu_eval_batch_size=32 \\\n",
    "    --gradient_accumulation_steps=1 \\\n",
    "    --max_steps=800 \\\n",
    "    --overwrite_output_dir --overwrite_cache --warmup_steps=120\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import (WEIGHTS_NAME, BertConfig,\n",
    "#                                   BertForSequenceClassification, BertTokenizer)\n",
    "\n",
    "\n",
    "# config_class, model_class, tokenizer_class = (BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "# config = config_class.from_pretrained(MODEL_DIR, num_labels=num_labels, finetuning_task=task_name)\n",
    "# tokenizer = tokenizer_class.from_pretrained(MODEL_DIR, do_lower_case=True)\n",
    "# model = model_class.from_pretrained(MODEL_DIR, from_tf=bool('.ckpt' in MODEL_DIR), config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}