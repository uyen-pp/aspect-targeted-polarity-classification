{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('python37': conda)",
   "metadata": {
    "interpreter": {
     "hash": "da2a2dd739486b98df65bba8b6599fd6aa55ff0b355cb46f8657e6daa711dc7a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This notebook is for testing of:\n",
    "- Data transformation from a \"cleaned, preprocessed, splited\" pickle file \n",
    "- Model loading\n",
    "- Model training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(PROJECT_DIR)\n",
    "sys.path.append(os.path.join(PROJECT_DIR, \"finetuning_and_classification\"))\n",
    "\n",
    "\n",
    "import utils\n",
    "from utils_glue import (compute_metrics, convert_examples_to_features,\n",
    "                        output_modes, processors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data load from pkl\n",
    "# Test function YNmilk_data_reform\n",
    "\n",
    "sentences, aspect_term_sentiments =utils.YNmilk_data_reform(\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\data_splited\\\\train.pickle\")\n",
    "\n",
    "sentences[0:5], aspect_term_sentiments[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "!python prepare_YNmilk_datasets.py \\\n",
    "    --files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\train.pickle\" \\\n",
    "    --output_dir data/transformed/yndata \\\n",
    "    --istrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepare_YNmilk_datasets.py \\\n",
    "--files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\validate.pickle\" \\\n",
    "--output_dir data/transformed/yndata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "MODEL_DIR = os.path.join(PROJECT_DIR, \"data/models/phobert_base/huggingface_phobert_base\")\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"data/transformed/yndata\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_DIR, \"YNData-ar-phobert-milk\")\n",
    "\n",
    "!python ./finetuning_and_classification/run_glue.py \\\n",
    "    --model_type=\"phobert\" \\\n",
    "    --model_name_or_path=C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/models/phobert_base/huggingface_phobert_base \\\n",
    "    --do_train \\\n",
    "    # --evaluate_during_training \\\n",
    "    --logging_steps 100 --save_steps 200 --task_name=\"atsc\" \\\n",
    "    --seed 42 \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir=\"$DATA_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --max_seq_length=128 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --per_gpu_eval_batch_size=32 \\\n",
    "    --gradient_accumulation_steps=1 \\\n",
    "    --max_steps=800 \\\n",
    "    --overwrite_output_dir --overwrite_cache --warmup_steps=120\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "\n",
    "\n",
    "# config_class, model_class, tokenizer_class = (BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "# config = config_class.from_pretrained(MODEL_DIR, num_labels=num_labels, finetuning_task=task_name)\n",
    "# tokenizer = tokenizer_class.from_pretrained(MODEL_DIR, do_lower_case=True)\n",
    "# model = model_class.from_pretrained(MODEL_DIR, from_tf=bool('.ckpt' in MODEL_DIR), config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepare_YNmilk_datasets.py \\\n",
    "--files \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\YNdata\\\\data_splited\\\\test.pickle\" \\\n",
    "--output_dir data/transformed/yndata/testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "!python ./finetuning_and_classification/run_glue.py \\\n",
    "    --model_type=\"phobert\" \\\n",
    "    --do_eval \\\n",
    "    --model_name_or_path=\"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification/data/models/finetuned_phobert4classification/\" \\\n",
    "    --task_name=\"atsc\" \\\n",
    "    --output_dir=\"C:/Users/Uyen/Documents/nlp/Thesis/aspect-targeted-polarity-classification/data/models/finetuned_phobert4classification/\"  \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir=./data/transformed/yndata/testdata \\\n",
    "    --max_seq_length=128 \\\n",
    "    --per_gpu_eval_batch_size=32 \\\n",
    "    --overwrite_output_dir --overwrite_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text parser\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "filepath = \"C:/Users/Uyen/Documents/nlp/thesis/aspect-targeted-polarity-classification/data/transformed/yndata/testdata/test.xml\"\n",
    "\n",
    "with open(filepath, 'rb') as f:\n",
    "    sentence_elements = ET.parse(f).getroot().iter('sentence')\n",
    "\n",
    "e_0 = list(sentence_elements)[0]\n",
    "e_0.find('text').text, e_0.find('aspectTerms').find('aspectTerm').get('term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-0365d4a76ec2245b\n",
      "Downloading and preparing dataset ar_dataset/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\Uyen\\.cache\\huggingface\\datasets\\ar_dataset\\default-0365d4a76ec2245b\\0.0.0\\ac6d68984bc8ec6167fa960dba1b07bd060b01dd4d3115652428832d6a179e17...\n",
      "                                ['Quà tặng']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False]\n",
      "['Chức_năng tiêu_hóa', 'Nguồn_gốc xuất_xứ', 'Hấp_thụ', 'Dinh_dưỡng']\n",
      "[False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Thành_phần']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False]\n",
      "['Phát_triển trí_não']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "['Nóng', 'Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chất_lượng chung']\n",
      "[False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Phân_phối']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False]\n",
      "['Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chất_lượng chung']\n",
      "[False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Hương_vị', 'Nhãn_hiệu']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chức_năng tiêu_hóa']\n",
      "[False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Chức_năng tiêu_hóa']\n",
      "[False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "['Quà tặng']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False]\n",
      "['Phát_triển thể_chất', 'Nhãn_hiệu']\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False]\n",
      "['Dinh_dưỡng']\n",
      "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Dataset ar_dataset downloaded and prepared to C:\\Users\\Uyen\\.cache\\huggingface\\datasets\\ar_dataset\\default-0365d4a76ec2245b\\0.0.0\\ac6d68984bc8ec6167fa960dba1b07bd060b01dd4d3115652428832d6a179e17. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader\n",
    "# !pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "data_dir=\"C:/Users/Uyen/Documents/nlp/YNdata/data_test\"\n",
    "train_file = os.path.join(data_dir, \"train.csv\")\n",
    "eval_file = os.path.join(data_dir, \"dev.csv\")\n",
    "\n",
    "dataset = load_dataset('C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\thesis\\\\aspect-targeted-polarity-classification\\\\finetuning_and_classification\\\\load_dataset_ar.py', data_files={\"train\": train_file, \"validation\": eval_file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[31, 3, 32, 29, 25, 2, 30, 6]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dataset[\"train\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# YN Data proposed\n",
    "YN_DATA_HOME = \"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\"\n",
    "\n",
    "# Clean YN Data dataframe pickle files\n",
    "YN_CLEAN = os.path.join(YN_DATA_HOME, \"splitted\")\n",
    "\n",
    "# YN Data transfrom to xml to be run_glue adaptable, for training\n",
    "YN_AR = os.path.join(YN_DATA_HOME, \"ar_dataset\")\n",
    "\n",
    "YN_ATSC = os.path.join(YN_DATA_HOME, \"atsc_dataset\")\n",
    "\n",
    "# YN Data transfrom to xml to be run_glue adaptable, for testing\n",
    "YN_TEST = os.path.join(YN_DATA_HOME, \"test\")\n",
    "\n",
    "# Aspect regconition model\n",
    "ASPECT_RECOGNITION_MODEL_DIR = \"./AR\"\n",
    "\n",
    "# Target-based sentiment classification model\n",
    "TARGETED_POLARITY_CLASSIFICATION_MODEL_DIR = \"/gdrive/MyDrive/Thesis/Models/TASC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = os.path.join(YN_AR, 'train.csv')\n",
    "EVAL_FILE = os.path.join(YN_AR, 'dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./finetuning_and_classification/my_run.py      --model_name_or_path=\"vinai/phobert-base\"     --do_train     --use_fast_tokenizer=False     --task_name=\"ar\"      --max_seq_length=\"256\"     --overwrite_cache=True     --pad_to_max_length=True     --train_file=\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\ar_dataset\\\\train.csv\"      --validation_file=\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\YNdata\\\\ar_dataset\\\\dev.csv\"      --output_dir=\"./AR_Model\" --overwrite_output_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}